{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahamruf/Fish-Diseses/blob/main/fish_efficientb3(ndam).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kWFOApmzhXFA"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow import keras\n",
        "import glob as gb\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D ,LeakyReLU\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI8fIPrDlCxT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sEgSZFSmUmb"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (299, 299) # resolution\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "IMG_SIZE = (299, 299)  # resolution\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Define the directories for training, testing, and validation\n",
        "train_directory = '/content/drive/MyDrive/dataset/Fishtrain'\n",
        "test_directory = '/content/drive/MyDrive/dataset/fishtest'\n",
        "valid_directory = '/content/drive/MyDrive/dataset/Fishvalid'\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_dataset = image_dataset_from_directory(\n",
        "    valid_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Optional: You can also specify the class names if you have a specific order for your classes\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "# Print class names\n",
        "print(\"Class Names:\", class_names)"
      ],
      "metadata": {
        "id": "TlChBFly4Jth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlyOO5AYmrHC"
      },
      "outputs": [],
      "source": [
        "class_names = train_dataset.class_names\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxaBFECcN1kG"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "Data_imbalance = []\n",
        "for folder in os.listdir(train_directory):\n",
        "    files = gb.glob(pathname=str(train_directory + \"/\" + folder +\"/*.*\"))\n",
        "    Data_imbalance.append(len(files))\n",
        "plt.figure(figsize=(13,7))\n",
        "sns.barplot(x=[\"Argulus\", \"Broken antennae and rostrum\", \"EUS\", \"Healthy Fish\",\"Redspot\",\"THE BACTERIAL GILL ROT\",\"Tail And Fin Rot\"], y=Data_imbalance, palette=\"rocket\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc_mQMgymtna"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import glob as gb\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Assuming you already have Data_imbalance calculated\n",
        "# If not, you should calculate it before using it in this code\n",
        "\n",
        "# Print the dataset class-wise count\n",
        "for folder, count in zip([\"Argulus\", \"Broken antennae and rostrum\", \"EUS\", \"Healthy Fish\",\"Redspot\",\"THE BACTERIAL GILL ROT\",\"Tail And Fin Rot\"], Data_imbalance):\n",
        "    print(f\"Class '{folder}': {count} samples\")\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(13,7))\n",
        "sns.barplot(x=[\"Argulus\", \"Broken antennae and rostrum\", \"EUS\", \"Healthy Fish\",\"Redspot\",\"THE BACTERIAL GILL ROT\",\"Tail And Fin Rot\"], y=Data_imbalance, palette=\"rocket\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnFrxbU2myHa"
      },
      "outputs": [],
      "source": [
        "total = 0\n",
        "for i in range(0,len(Data_imbalance)) :\n",
        "    total +=Data_imbalance[i]\n",
        "\n",
        "weight_for_0 = (1 / Data_imbalance[0]) * (total / 7.0)\n",
        "weight_for_1 = (1 / Data_imbalance[1]) * (total / 7.0)\n",
        "weight_for_2 = (1 / Data_imbalance[2]) * (total / 7.0)\n",
        "weight_for_3 = (1 / Data_imbalance[3]) * (total / 7.0)\n",
        "weight_for_4 = (1 / Data_imbalance[3]) * (total / 7.0)\n",
        "weight_for_5 = (1 / Data_imbalance[3]) * (total / 7.0)\n",
        "weight_for_6 = (1 / Data_imbalance[3]) * (total / 7.0)\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3,4: weight_for_4,5: weight_for_5,6: weight_for_6}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "print('Weight for class 2: {:.2f}'.format(weight_for_2))\n",
        "print('Weight for class 3: {:.2f}'.format(weight_for_3))\n",
        "print('Weight for class 4: {:.2f}'.format(weight_for_4))\n",
        "print('Weight for class 5: {:.2f}'.format(weight_for_5))\n",
        "print('Weight for class 6: {:.2f}'.format(weight_for_6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgzmxi6_m3d6"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "  tf.keras.layers.RandomZoom(0.2),\n",
        "  tf.keras.layers.RandomHeight(0.2),\n",
        "  tf.keras.layers.RandomWidth(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T9iX4nrm6ox"
      },
      "outputs": [],
      "source": [
        "for image, _ in train_dataset.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = image[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "        plt.imshow(augmented_image[0] / 255)\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUD1D4JEm8fK"
      },
      "outputs": [],
      "source": [
        "augmented_count = 0\n",
        "\n",
        "for image, _ in train_dataset:\n",
        "    augmented_images = data_augmentation(image)\n",
        "    augmented_count += augmented_images.shape[0]  # Count the number of augmented images\n",
        "\n",
        "original_count = len(train_dataset) * BATCH_SIZE  # Total number of original images\n",
        "total_count = original_count + augmented_count    # Total number of images after augmentation\n",
        "\n",
        "increase_percentage = ((total_count - original_count) / original_count) * 100\n",
        "\n",
        "print(f\"Original image count: {original_count}\")\n",
        "print(f\"Augmented image count: {augmented_count}\")\n",
        "print(f\"Total image count after augmentation: {total_count}\")\n",
        "print(f\"Increase in image count: {increase_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input"
      ],
      "metadata": {
        "id": "fyx0Kmspew3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdV2ld3hnBHS"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE = IMG_SIZE +(3,)\n",
        "# Load the EfficientNetB0 model pre-trained on ImageNet\n",
        "base_model =tf.keras.applications.EfficientNetB3(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arSLUFrjnDSq"
      },
      "outputs": [],
      "source": [
        "type(base_model)\n",
        "preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTR4mUPYnF7C"
      },
      "outputs": [],
      "source": [
        "nb_layers = len(base_model.layers)\n",
        "print(\"Numbers of Layers =\" , nb_layers)\n",
        "print(base_model.layers[nb_layers - 2].name)  # pre- Last name\n",
        "print(base_model.layers[nb_layers - 1].name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfnkhQg2nG7i"
      },
      "outputs": [],
      "source": [
        "# iterate over first batch (32 image) in trainset\n",
        "image_batch, label_batch = next(iter(train_dataset))  # 32 image arrays\n",
        "feature_batch = base_model(image_batch)  # run the model on those 32 image (base model with its 1000 causes classification)\n",
        "print(feature_batch.shape)  # 32 for number of images in this batch and 1000 for classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ69NIMxnLHE"
      },
      "outputs": [],
      "source": [
        "def fish_densenet( image_shape=IMG_SIZE):\n",
        "    ''' Define a tf.keras model for multi-class classification out of the InceptionV3 model '''\n",
        "    image_shape = image_shape + (3,)\n",
        "\n",
        "    resnet_model = tf.keras.applications.EfficientNetB3(input_shape=IMG_SHAPE, include_top= False,weights='imagenet')\n",
        "    resnet_model.trainable = True\n",
        "    for layer in resnet_model.layers[0 : 291]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(image_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = preprocess_input(inputs)\n",
        "    x = resnet_model(x , training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    prediction_layer = tf.keras.layers.Dense(7 ,activation = \"softmax\")\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edhECGvMphs7"
      },
      "outputs": [],
      "source": [
        "fish = fish_densenet(IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEjDTaGznOOa"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.001\n",
        "fish.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
        "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                           metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model_filepath=\"/content/drive/MyDrive/EfficientNetB3-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = model_filepath ,\n",
        "    monitor ='val_accuracy',\n",
        "    mode = 'max' ,\n",
        "    save_best_only =True ,\n",
        "     verbose = 1\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "2nOW8GuyvwUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bD1EK4inRa6"
      },
      "outputs": [],
      "source": [
        "history = fish.fit(train_dataset , verbose=2 , epochs=50 ,class_weight=class_weight,\n",
        "                               validation_data=valid_dataset , use_multiprocessing= True,callbacks =[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjCEYgWuniLa"
      },
      "outputs": [],
      "source": [
        "fish.evaluate(test_dataset , verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0tz0ZdZnkYa"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "for images, labels in test_dataset.take(1):\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "        im2 = images[i].numpy().astype(\"uint8\")\n",
        "        img2 = tf.expand_dims(im2, 0)\n",
        "        predict = fish.predict(img2)\n",
        "        predicted= class_names[np.argmax(predict)]\n",
        "        actual = class_names [labels[i].numpy().astype(\"uint8\")]\n",
        "        if (actual == predicted):\n",
        "            plt.title(predicted, fontsize=10, color= 'blue', pad=15);\n",
        "        else :\n",
        "            plt.title(actual, fontsize=10, color= 'red' ,pad=15);\n",
        "        plt.subplots_adjust(left=0.1,bottom=0.1, right=0.9,\n",
        "                            top=0.9, wspace=0.4,hspace=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-QN_t9XnnHi"
      },
      "outputs": [],
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(17, 12))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,3.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxzeEBbQnpDM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Initialize empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation dataset and make predictions\n",
        "for images, labels in test_dataset:\n",
        "    predictions = fish.predict(images)\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "# Binarize the true and predicted labels\n",
        "true_labels_bin = label_binarize(true_labels, classes=np.unique(true_labels))\n",
        "predicted_labels_bin = label_binarize(predicted_labels, classes=np.unique(predicted_labels))\n",
        "\n",
        "# Compute class-specific ROC AUC values\n",
        "roc_auc_per_class = []\n",
        "for i in range(len(class_names)):\n",
        "    roc_auc = roc_auc_score(true_labels_bin[:, i], predicted_labels_bin[:, i])\n",
        "    roc_auc_per_class.append(roc_auc)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "# Print class-specific ROC AUC values\n",
        "for i in range(len(class_names)):\n",
        "    print(f'ROC AUC for {class_names[i]}: {roc_auc_per_class[i]:.4f}')\n",
        "\n",
        "# Print the classification report\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5jU1YJYOWuE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix EfficientNetB3)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPALI4JO-SNu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Initialize empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation dataset and make predictions\n",
        "for images, labels in test_dataset:\n",
        "    predictions = fish.predict(images)\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "# Binarize the true and predicted labels\n",
        "true_labels_bin = label_binarize(true_labels, classes=np.unique(true_labels))\n",
        "predicted_labels_bin = label_binarize(predicted_labels, classes=np.unique(predicted_labels))\n",
        "\n",
        "# Compute class-specific ROC AUC values\n",
        "roc_auc_per_class = []\n",
        "for i in range(len(class_names)):\n",
        "    roc_auc = roc_auc_score(true_labels_bin[:, i], predicted_labels_bin[:, i])\n",
        "    roc_auc_per_class.append(roc_auc)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "# Plot class-specific ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "    fpr, tpr, _ = roc_curve(true_labels_bin[:, i], predicted_labels_bin[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f}) for {class_names[i]}')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for Multiclass Fish Diseases Classification EfficientNetB3')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Print class-specific ROC AUC values\n",
        "for i in range(len(class_names)):\n",
        "    print(f'ROC AUC for {class_names[i]}: {roc_auc_per_class[i]:.4f}')\n",
        "\n",
        "# Print the classification report\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.4f}')"
      ],
      "metadata": {
        "id": "fMl2Zi2JZFlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "import numpy as np\n",
        "\n",
        "# Assuming true_labels and predicted_labels are your multiclass labels\n",
        "mcc_values = [matthews_corrcoef(true_labels == i, predicted_labels == i) for i in np.unique(true_labels)]\n",
        "\n",
        "average_mcc = np.mean(mcc_values)\n",
        "print(f'Average Matthews Correlation Coefficient for Multiclass: {average_mcc:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "LYJFfuKPZcQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ApcJEpVOZ1rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['blue', 'green', 'red', 'purple', 'orange','yellow','black']"
      ],
      "metadata": {
        "id": "ADCfFHf5Z6v2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj06ahA+IEJfFsRqLj+3YP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}